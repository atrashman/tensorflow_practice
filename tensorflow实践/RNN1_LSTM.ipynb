{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\models\\tutorials\\rnn\\ptb\\reader.py:120: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\input.py:318: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-4-3cb75664a330>:202: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "In iteration: 1\n",
      "After 0 steps,perplexity is 9970.101\n",
      "After 100 steps,perplexity is 1404.397\n",
      "After 200 steps,perplexity is 1065.800\n",
      "After 300 steps,perplexity is 891.973\n",
      "After 400 steps,perplexity is 781.927\n",
      "After 500 steps,perplexity is 705.665\n",
      "After 600 steps,perplexity is 648.875\n",
      "After 700 steps,perplexity is 599.748\n",
      "After 800 steps,perplexity is 555.520\n",
      "After 900 steps,perplexity is 520.728\n",
      "After 1000 steps,perplexity is 494.283\n",
      "After 1100 steps,perplexity is 468.808\n",
      "After 1200 steps,perplexity is 447.755\n",
      "After 1300 steps,perplexity is 428.562\n",
      "Epoch: 1 Validation Perplexity: 244.345\n",
      "In iteration: 2\n",
      "After 0 steps,perplexity is 352.298\n",
      "After 100 steps,perplexity is 246.086\n",
      "After 200 steps,perplexity is 251.324\n",
      "After 300 steps,perplexity is 251.564\n",
      "After 400 steps,perplexity is 248.238\n",
      "After 500 steps,perplexity is 245.646\n",
      "After 600 steps,perplexity is 244.967\n"
     ]
    }
   ],
   "source": [
    "#RNN模型一般直接用来进行长期记忆计算\n",
    "#LSTM模型代码  \n",
    "#两层LSTM  一层全连接 ，输出为一个10000维的向量\n",
    "#PTB数据集   完整代码如下\n",
    "#参考https://blog.csdn.net/White_Idiot/article/details/78881261  \n",
    "#http://dataunion.org/30472.html此网站极其详细介绍tf的RNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.tutorials.rnn.ptb import reader\n",
    "import numpy as np\n",
    "\n",
    "#读文件\n",
    "DATA_PATH = r'C:\\Users\\abc\\Downloads\\simple-examples\\simple-examples\\data'#存放的位置\n",
    "train_data ,valid_data,test_data ,_= reader.ptb_raw_data(DATA_PATH)#每个单词转化成数字格式了\n",
    "#print (len(train_data))#txt总共929589个单词  句子结束标识符为2\n",
    "\n",
    "#参数设置\n",
    "VOCAB_SIZE = 10000  #词典单词数\n",
    "\n",
    "#训练超参数\n",
    "TRAIN_BATCH_SIZE = 20 #batch大小   \n",
    "LEARNING_RATE = 1.0   #学习率\n",
    "TRAIN_NUM_STEP = 35  #截断长度     时间长度\n",
    "\n",
    "#测试不需要截断\n",
    "EVAL_BATCH_SIZE = 1 #batch大小\n",
    "EVAL_NUM_STEP = 1  #截断长度\n",
    "\n",
    "#神经网络参数\n",
    "HIDDEN_SIZE = 200  #LSTM隐藏层节点\n",
    "NUM_EPOCH = 20    #训练轮数\n",
    "KEEP_PROB = 0.5   #节点不被droupout概率\n",
    "MAX_GRAD_NORM = 5  #梯度膨胀参数\n",
    "NUM_LAYERS = 2  #LSTM结构层数\n",
    "\n",
    "# 通过PTBModel描述模型，方便维护循环神经网络中的状态\n",
    "class PTBModel():\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        # 记录batch和截断长度\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # 定义输入层\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        # 定义预期输出\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        # 定义LSTM为使用dropout的两层网络\n",
    "        num_layer_size = [HIDDEN_SIZE, HIDDEN_SIZE]\n",
    "        #lstm_cell = tf.contrib.rnn.BasicLSTMCell(HIDDEN_SIZE)\n",
    "        lstm_cell = [tf.nn.rnn_cell.BasicLSTMCell(num_units=size) for size in num_layer_size]\n",
    "        if is_training:\n",
    "            for i in range(len(num_layer_size)):\n",
    "                lstm_cell[i] = tf.contrib.rnn.DropoutWrapper(\n",
    "                        lstm_cell[i], output_keep_prob=KEEP_PROB)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                                       lstm_cell, state_is_tuple=True)\n",
    "#注意使用多层cell时必须用下面的写法而不能直接简单地定义一个单层cell然后*num_layers，因为这样会导致各cell层共享权重\n",
    "#tf.nn.rnn_cell.LSTMCell\n",
    "        \n",
    "\n",
    "        # 初始化state\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)###？？？？？？？为什么是batch_size个\n",
    "\n",
    "        # 将单词ID转为单词向量。每个单词都是HIDDEN_SIZE维\n",
    "        embedding = tf.get_variable('embedding', [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "\n",
    "        # 将原本batch_size*num_steps的输入层转化为batch_size*num_steps*HIDDEN_SIZE的三维0/1矩阵\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)#One-hot型的矩阵相乘，可以简化为查表操作\n",
    "\n",
    "        # 只在训练时使用dropout\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, KEEP_PROB)\n",
    "\n",
    "        # 定义输出列表\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()#管理传给get_variable()的变量名称的作用域  embedding使用\n",
    "                cell_output, state = cell(inputs[:, time_step, :],\n",
    "                                          state)  # 将当前时刻的数据和状态传入LSTM，需要state\n",
    "                outputs.append(cell_output)  # 将当前输出加入输出列表\n",
    "\n",
    "        # 将输出列表展开成[batch,hidden_size*num_steps]\n",
    "        # 再reshape成[batch*num_steps,hidden_size]\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])#concat指定维度重新拼接，本来是batch*hidden_size为元素的len为num_step的列表\n",
    "\n",
    "        # 将输出传入全连接层，每个时刻的输出都是长度为VOCAB_SIZE的数组\n",
    "        weight = tf.get_variable('weight', [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        bias = tf.get_variable('bias', [VOCAB_SIZE])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "\n",
    "        # 定义交叉熵损失函数，sequence_loss_by_example计算一个序列的交叉熵的和\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],  # 预测结果\n",
    "            [tf.reshape(self.targets, [-1])\n",
    "             ],  # 预期结果。将[batch_size,num_steps]压缩成一维\n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)\n",
    "             ]  # 损失的权重。这里所有的权重都为1，表示不同batch和不同时刻的重要程度都一样\n",
    "        )\n",
    "\n",
    "        # 计算得到每个batch的平均损失\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "\n",
    "        # 只在训练时反向传播\n",
    "        if not is_training:\n",
    "            return\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.cost, trainable_variables),#cost对所有训练变量求导\n",
    "            MAX_GRAD_NORM)  # 控制梯度大小。避免梯度膨胀# 分界点为MAX_GRAD_NORM  小于则加，大于则减\n",
    "\n",
    "        # 定义优化方法\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "\n",
    "        # 定义训练步骤\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables))\n",
    "\n",
    "\n",
    "def run_epoch(session, model, data_queue, train_op, output_log, epoch_size):#model是一个类，方法和属性之间相互利用\n",
    "    # 计算perplexity的辅助变量\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)#run lstm得到output_state\n",
    "    #feed机制可以对已经赋值和未赋值（placeholder）都进行赋值\n",
    "    # 使用当前数据训练或测试模型\n",
    "    for step in range(epoch_size):\n",
    "        # 生成输入和答案\n",
    "        feed_dict = {}\n",
    "        x, y = session.run(data_queue)\n",
    "        feed_dict[model.input_data] = x\n",
    "        feed_dict[model.targets] = y\n",
    "\n",
    "        # 将状态转为字典\n",
    "        for i, (c, h) in enumerate(model.initial_state):####len = batch  #每次迭代都把上过一次每层的statefeed过去，目的是复用LSTM模型那个类\n",
    "            feed_dict[c] = state[i].c#输出\n",
    "            feed_dict[h] = state[i].h#向下一个时间传递 直接feed给final_state，state可以得到output进而得到cost\n",
    "        #print(len(feed_dict))输出是6  2个是input和target  剩下四个分别是每层的output和state\n",
    "        #feed_dict 有数据队伍，目标队伍，LSTM模型两层的的输出和传递状态\n",
    "        # 获取损失值和下一个状态\n",
    "        cost, state, _ = session.run(\n",
    "            [model.cost, model.final_state, train_op], feed_dict=feed_dict\n",
    "        )  # 在当前batch上运行train_op并计算损失值。交叉熵损失函数计算的是下一个单词为给定单词的概率\n",
    "        #feed output是给全连接神经网络\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        # 训练时输出日志\n",
    "        if output_log and step % 100 == 0:\n",
    "            print('After %d steps,perplexity is %.3f' %\n",
    "                  (step, np.exp(total_costs / iters)))\n",
    "\n",
    "    return np.exp(total_costs / iters)\n",
    "\n",
    "#一个epoch遍历所有训练数据，一个batch更新一次模型\n",
    "def main(_):\n",
    "    # 原始数据\n",
    "    train_data, valid_data, test_data, _ = reader.ptb_raw_data(DATA_PATH)\n",
    "\n",
    "    # 计算一个epoch需要训练的次数\n",
    "    train_data_len = len(train_data)  # 数据集的大小\n",
    "    train_batch_len = train_data_len // TRAIN_BATCH_SIZE  # batch的个数\n",
    "    train_epoch_size = (train_batch_len - 1) // TRAIN_NUM_STEP  # 该epoch的训练次数\n",
    "\n",
    "    valid_data_len = len(valid_data)\n",
    "    valid_batch_len = valid_data_len // EVAL_BATCH_SIZE\n",
    "    valid_epoch_size = (valid_batch_len - 1) // EVAL_NUM_STEP\n",
    "\n",
    "    test_data_len = len(test_data)\n",
    "    test_batch_len = test_data_len // EVAL_BATCH_SIZE\n",
    "    test_epoch_size = (test_batch_len - 1) // EVAL_NUM_STEP\n",
    "    \n",
    "    # 定义初始化函数\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "\n",
    "    # 定义训练用的模型\n",
    "    with tf.variable_scope(\n",
    "            'language_model', reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "     # 定义评估用的模型\n",
    "    with tf.variable_scope(\n",
    "            'language_model', reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)    \n",
    "    \n",
    "    # 生成数据队列，必须放在开启多线程之前\n",
    "    train_queue = reader.ptb_producer(train_data, train_model.batch_size,\n",
    "                                      train_model.num_steps)\n",
    "    valid_queue = reader.ptb_producer(valid_data, eval_model.batch_size,\n",
    "                                      eval_model.num_steps)\n",
    "    test_queue = reader.ptb_producer(test_data, eval_model.batch_size,\n",
    "                                     eval_model.num_steps)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # 开启多线程从而支持ptb_producer()使用tf.train.range_input_producer()\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)#多线程只要指定sess和coord就行了\n",
    "\n",
    "        # 使用训练数据训练模型\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print('In iteration: %d' % (i + 1))\n",
    "            run_epoch(sess, train_model, train_queue, train_model.train_op,\n",
    "                      True, train_epoch_size)  # 训练模型\n",
    "            valid_perplexity = run_epoch(sess, eval_model, valid_queue,\n",
    "                                         tf.no_op(), False,\n",
    "                                         valid_epoch_size)  # 使用验证数据评估模型\n",
    "            print('Epoch: %d Validation Perplexity: %.3f' % (i + 1,\n",
    "                                                             valid_perplexity))\n",
    "\n",
    "        # 使用测试数据测试模型\n",
    "        test_perplexity = run_epoch(sess, eval_model, test_queue,\n",
    "                                    tf.no_op(), False, test_epoch_size)\n",
    "        print('Test Perplexity: %.3f' % test_perplexity)\n",
    "\n",
    "        # 停止所有线程\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n"
     ]
    }
   ],
   "source": [
    "print(train_data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-198e1f019572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "#这里是测试新版本tensorflow函数\n",
    "#按batch大小截断  函数为ptb_producer\n",
    "#batch为4  截断长度为5\n",
    "# result  = reader.ptb_producer(train_data,4,5)\n",
    "# with tf.Session() as sess:\n",
    "#     tf.global_variables_initializer().run()\n",
    "\n",
    "#     # 开启多线程\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "#     # 读取前两个batch，其中包括每个时刻的输入和对应的答案，ptb_producer()会自动迭代\n",
    "#     for i in range(2):\n",
    "#         x, y = sess.run(result)\n",
    "#         print('x:', x)\n",
    "#         print('y:', y)\n",
    "\n",
    "#     # 关闭多线程\n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.1 1.2 1.3]\n",
      "  [2.1 2.2 2.3]\n",
      "  [3.1 3.2 3.3]]\n",
      "\n",
      " [[3.1 3.2 3.3]\n",
      "  [2.1 2.2 2.3]\n",
      "  [1.1 1.2 1.3]]]\n"
     ]
    }
   ],
   "source": [
    "#tf.nn.embedding_lookup测试\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "input_ids = tf.placeholder(dtype=tf.int32, shape=[None,3])\n",
    "embedding = a = np.asarray([[0.1, 0.2, 0.3], [1.1, 1.2, 1.3], [2.1, 2.2, 2.3], [3.1, 3.2, 3.3], [4.1, 4.2, 4.3]])\n",
    "input_embedding = tf.nn.embedding_lookup(embedding, input_ids)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# print(embedding.eval())\n",
    "print(sess.run(input_embedding, feed_dict={input_ids: [[1, 2, 3],[ 3, 2, 1]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None) \n",
    "#通过权重梯度的总和的比率来截取多个张量的值。 \n",
    "#t_list 是梯度张量， clip_norm 是截取的比率, 这个函数返回截取过的梯度张量和一个所有张量的全局范数。\n",
    "\n",
    "#g = tf.gradients(y,x)  #交叉求导数\n",
    "\n",
    "#RNNcell是可call的   例如我们有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)\n",
    "#再调用一次call(x2, h1)就可以得到(output2, h2)：\n",
    "  \n",
    "#ptb_producer()和slice切分数据集介绍的介绍，可以看出input应该是batch_size*epoch_size的  #https://blog.csdn.net/a343902152/article/details/54429096\n",
    "#上面的网站还有个词汇表函数_build_vocab   用来构建词汇表\n",
    "#tf.slice()到底怎么切  https://www.jianshu.com/p/71e6ef6c121b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, _ = reader.ptb_raw_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9971"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

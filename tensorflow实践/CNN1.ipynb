{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d82368d6b81d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#第三个参数是一个长度为4的数组 格式为[1, , ,1] 表示filter对输入的操作\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#第四个参数是填充方法（padding） 为'SAME' （全0）和 'VALID'（不添加）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilter_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m#由于每一次层的节点偏置项相同，利用tf.nn.bias_add函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_' is not defined"
     ]
    }
   ],
   "source": [
    "#卷积：\n",
    "#参数为  名字 前两个表示过滤器尺寸，第三个是当前层深度 第四个是过滤器深度（也就是过滤器个数，也是下层的深度）\n",
    "filter_writer = tf.get_variable('weights',[5,5,3,16],initializer = tf.truncated_normal_initializer(stddev = 0.1)) \n",
    "biases = tf.get_variable('biases',[16],initializer = tf.constant_initializer(0.1))\n",
    "#tf.nn.conv2d计算卷积\n",
    "#第一个参数是batch   input[0, , , ]表示第一章图片 input[1, , , ]表示第二张图片 。。。\n",
    "#第二个参数是bias  \n",
    "#第三个参数是一个长度为4的数组 格式为[1, , ,1] 表示filter对输入的操作\n",
    "#第四个参数是填充方法（padding） 为'SAME' （全0）和 'VALID'（不添加）\n",
    "conv = tf.nn.conv2d(input_,filter_weights,strides = [1,1,1,1],padding = 'SAME')\n",
    "#由于每一次层的节点偏置项相同，利用tf.nn.bias_add函数\n",
    "output = tf.nn.bias_add(conv,biases)\n",
    "#计算结果用relu激活\n",
    "active_conv = tf.nn.relu(output)\n",
    "\n",
    "#池化 (池化过滤器只影响一个深度的节点)\n",
    "#平均值或者最大值池化\n",
    "#tf.nn.max_pool  tf.nn.avg_pool  也有padding参数\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5c2a771daf9a>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\abc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#LeNet-5模型实现识别mnist\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#一些经验：一般下一层卷积的filter数会乘2\n",
    "\n",
    "#训练参数：\n",
    "BATCH_SIZE = 100#随机梯度下降法\n",
    "learning_rate_origin = 0.8 #初始学习率\n",
    "learning_rate_decay = 0.99#衰减率\n",
    "lambda_ = 0.0001#正则化lambda\n",
    "train_srep = 30000#训练轮次\n",
    "moving_average_decay = 0.99\n",
    "\n",
    "#卷积池化卷积池化 再用一个卷积层（此层的过滤器只有一个）把输出铺平  全连接\n",
    "\n",
    "#先建立placehoder\n",
    "#x = tf.placeholder(tf.float32,[BATCH_SIZE,len_,width_,deepth],name = 'x-input')#RGB格式深度为5\n",
    "\n",
    "#模型参数\n",
    "INPUT_NODE = 784 #28*28\n",
    "OUTPUT_NODE = 10\n",
    "IMAGE_SIZE = 28 #图片尺寸\n",
    "NUM_CHANNELS = 1 #\n",
    "NUM_LABELS = 10  #标签数量\n",
    "#第一层卷积\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 5\n",
    "#第二层卷积\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 5\n",
    "#全连接节点个数\n",
    "FC_SIZE = 512\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training steps, loss is 4.94549.\n",
      "After 1001 training steps, loss is 0.876469.\n",
      "After 2001 training steps, loss is 0.800536.\n"
     ]
    }
   ],
   "source": [
    "def inference(input_tensor, train, regularizer):\n",
    "    # 声明第一层卷积层的变量并实现前向传播过程\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable(\n",
    "            'weight', [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\n",
    "            'bias', [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # 使用边长为5，深度为32的过滤器，过滤器步长为1，使用全0填充\n",
    "        conv1 = tf.nn.conv2d(\n",
    "            input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    # 实现第二层池化层的前向传播。使用最大池化层，池化层过滤器的边长为2，步长为2，使用全0填充\n",
    "    with tf.variable_scope('layer2-pool1'):\n",
    "        pool1 = tf.nn.max_pool(\n",
    "            relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 声明第三层卷积层的变量并实现前向传播\n",
    "    with tf.variable_scope('layer3-conv2'):\n",
    "        conv2_weights = tf.get_variable(\n",
    "            'weight', [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\n",
    "            'bias', [CONV2_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # 使用边长为5，深度为64的过滤器，过滤器步长为1，使用全0填充\n",
    "        conv2 = tf.nn.conv2d(\n",
    "            pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    # 实现第四层池化层的前向传播\n",
    "    with tf.variable_scope('layer4-pool2'):\n",
    "        pool2 = tf.nn.max_pool(\n",
    "            relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    #得到pool2层的矩阵大小\n",
    "    # 将第四层池化层的输出转化为第五层全连接层的输入格式\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n",
    "\n",
    "    # 声明第五层全连接层的变量并前向传播\n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable(\n",
    "            'weight', [nodes, FC_SIZE],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        fc1_biases = tf.get_variable(\n",
    "            'bias', [FC_SIZE], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        # 只有全连接层的权重加入正则化\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "\n",
    "        # 只有训练的时候才使用dropout\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "    # 声明第六层的变量并前向传播\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable(\n",
    "            'weight', [FC_SIZE, NUM_LABELS],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        fc2_biases = tf.get_variable(\n",
    "            'bias', [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "\n",
    "    return logit #返回向前传播结果\n",
    "\n",
    "import os\n",
    "\n",
    "# 优化方法参数\n",
    "LEARNING_RATE_BASE = 0.01  # 基础学习率\n",
    "LEARNING_RATE_DECAY = 0.99  # 学习率的衰减率\n",
    "REGULARIZATION_RATE = 0.0001  # 正则化项在损失函数中的系数\n",
    "MOVING_AVERAGE_DECAY = 0.99  # 滑动平均衰减率\n",
    "\n",
    "# 训练参数\n",
    "BATCH_SIZE = 100  # 一个训练batch中的图片数\n",
    "TRAINING_STEPS = 30000  # 训练轮数\n",
    "\n",
    "# 模型保存的路径和文件名\n",
    "MODEL_SAVE_PATH = 'model/'\n",
    "MODEL_NAME = 'lenet5.ckpt'\n",
    "\n",
    "\n",
    "def train(mnist):\n",
    "    # 实现模型\n",
    "    x = tf.placeholder(\n",
    "        tf.float32, [\n",
    "            BATCH_SIZE, IMAGE_SIZE,IMAGE_SIZE,\n",
    "            NUM_CHANNELS\n",
    "        ],\n",
    "        name='x-input')  # 输入层\n",
    "    y_ = tf.placeholder(\n",
    "        tf.float32, [None, OUTPUT_NODE], name='y-input')  # 标签\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(\n",
    "        REGULARIZATION_RATE)  # 定义L2正则化损失函数\n",
    "    y = inference(x, True, regularizer)  # 输出层\n",
    "\n",
    "    # 存储训练轮数，设置为不可训练\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 设置滑动平均方法\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY, global_step)  # 定义滑动平均类\n",
    "    variable_averages_op = variable_averages.apply(\n",
    "        tf.trainable_variables())  # 在所有可训练的变量上使用滑动平均\n",
    "\n",
    "    # 设置指数衰减法\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY)\n",
    "\n",
    "    # 最小化损失函数\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=y, labels=tf.argmax(y_, 1))  # 计算每张图片的交叉熵\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)  # 计算当前batch中所有图片的交叉熵平均值\n",
    "    loss = cross_entropy_mean + tf.add_n(\n",
    "        tf.get_collection('losses'))  # 总损失等于交叉熵损失和正则化损失的和\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "        loss, global_step=global_step)  # 优化损失函数\n",
    "\n",
    "    # 同时反向传播和滑动平均\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 初始化持久化类\n",
    "    #saver = tf.train.Saver()\n",
    "\n",
    "    # 开始训练\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化所有变量\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # 迭代训练\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 产生该轮batch\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            xs = np.reshape(\n",
    "                xs, (BATCH_SIZE, IMAGE_SIZE,IMAGE_SIZE,\n",
    "                     NUM_CHANNELS))  # 将MNIST数据格式转为四维矩阵\n",
    "            _, loss_value, step = sess.run(\n",
    "                [train_op, loss, global_step], feed_dict={\n",
    "                    x: xs,\n",
    "                    y_: ys\n",
    "                })\n",
    "\n",
    "            # 每1000轮保存一次模型\n",
    "            if i % 1000 == 0:\n",
    "                # 输出训练情况\n",
    "                print('After %d training steps, loss is %g.' % (step,\n",
    "                                                                loss_value))\n",
    "\n",
    "                # 保存当前模型\n",
    "                #saver.save(\n",
    "                #    sess,\n",
    "                #   os.path.join(MODEL_SAVE_PATH, MODEL_NAME),\n",
    "                #    global_step=global_step)\n",
    "\n",
    "\n",
    "# 主程序入口\n",
    "def main(argv=None):\n",
    "\n",
    "    train(mnist)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "#主程序\n",
    "def main(argv = None):\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ =='__main__':\n",
    "     tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist.test.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
